{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        return output, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, model_dim):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        assert model_dim % num_heads == 0\n",
    "        \n",
    "        self.depth = model_dim // num_heads\n",
    "        self.WQ = nn.Linear(model_dim, model_dim)\n",
    "        self.WK = nn.Linear(model_dim, model_dim)\n",
    "        self.WV = nn.Linear(model_dim, model_dim)\n",
    "        self.linear = nn.Linear(model_dim, model_dim)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        Q = self.split_heads(self.WQ(Q), batch_size)\n",
    "        K = self.split_heads(self.WK(K), batch_size)\n",
    "        V = self.split_heads(self.WV(V), batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        scaled_attention = scaled_attention.transpose(1, 2).contiguous()\n",
    "        scaled_attention = scaled_attention.view(batch_size, -1, self.model_dim)\n",
    "        \n",
    "        output = self.linear(scaled_attention)\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, ff_dim):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.linear2 = nn.Linear(ff_dim, model_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads, model_dim)\n",
    "        self.ffn = PositionwiseFeedForward(model_dim, ff_dim)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(model_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        out1 = self.layernorm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + self.dropout2(ffn_output))\n",
    "        \n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_layers, num_heads, ff_dim, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
    "        self.pos_encoding = self.positional_encoding(max_len=5000, model_dim=model_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(model_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def positional_encoding(self, max_len, model_dim):\n",
    "        pos_enc = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_enc.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(num_heads, model_dim)\n",
    "        self.mha2 = MultiHeadAttention(num_heads, model_dim)\n",
    "        self.ffn = PositionwiseFeedForward(model_dim, ff_dim)\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(model_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(model_dim)\n",
    "        self.layernorm3 = nn.LayerNorm(model_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        attn1, _ = self.mha1(x, x, x, look_ahead_mask)\n",
    "        out1 = self.layernorm1(x + self.dropout1(attn1))\n",
    "        \n",
    "        attn2, _ = self.mha2(out1, enc_output, enc_output, padding_mask)\n",
    "        out2 = self.layernorm2(out1 + self.dropout2(attn2))\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        out3 = self.layernorm3(out2 + self.dropout3(ffn_output))\n",
    "        \n",
    "        return out3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, model_dim, num_layers, num_heads, ff_dim, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, model_dim)\n",
    "        self.pos_encoding = self.positional_encoding(max_len=5000, model_dim=model_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(model_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(model_dim, output_dim)\n",
    "    \n",
    "    def positional_encoding(self, max_len, model_dim):\n",
    "        pos_enc = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_enc.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:, :seq_len, :].to(x.device)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, look_ahead_mask, padding_mask)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, model_dim, num_layers, num_heads, ff_dim, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, model_dim, num_layers, num_heads, ff_dim, dropout)\n",
    "        self.decoder = Decoder(output_dim, model_dim, num_layers, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_tgt_mask=None):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        output = self.decoder(tgt, enc_output, tgt_mask, src_tgt_mask)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example: classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some data\n",
    "data = [\n",
    "    (\"I loved this movie, it was fantastic!\", 1),\n",
    "    (\"Horrible film, would not recommend.\", 0),\n",
    "    (\"The plot was very boring and predictable.\", 0),\n",
    "    (\"Great acting and amazing cinematography.\", 1),\n",
    "    (\"I really enjoyed this film.\", 1),\n",
    "    (\"Waste of time, I hated it.\", 0)\n",
    "]\n",
    "\n",
    "# \n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower().strip().split()\n",
    "    return text\n",
    "\n",
    "vocab = Counter()\n",
    "for sentence, label in data:\n",
    "    vocab.update(preprocess_text(sentence))\n",
    "\n",
    "vocab = {word: idx + 2 for idx, (word, _) in enumerate(vocab.items())}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "\n",
    "def sentence_to_indices(sentence, vocab):\n",
    "    return [vocab.get(word, vocab[\"<unk>\"]) for word in preprocess_text(sentence)]\n",
    "\n",
    "# index\n",
    "indexed_data = [(sentence_to_indices(sentence, vocab), label) for sentence, label in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data, vocab, max_len=20):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.data[idx]\n",
    "        sentence = sentence[:self.max_len]\n",
    "        sentence += [self.vocab[\"<pad>\"]] * (self.max_len - len(sentence))\n",
    "        return torch.tensor(sentence), torch.tensor(label)\n",
    "\n",
    "dataset = TextClassificationDataset(indexed_data, vocab)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForClassification(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_layers, num_heads, ff_dim, num_classes, dropout=0.1):\n",
    "        super(TransformerForClassification, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, model_dim, num_layers, num_heads, ff_dim, dropout)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "    \n",
    "    def forward(self, src, mask=None):\n",
    "        enc_output = self.encoder(src, mask)\n",
    "        #\n",
    "        enc_output = enc_output[:, 0, :]\n",
    "        output = self.fc(enc_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 3.441915273666382\n",
      "Epoch 2/100, Loss: 0.3453487157821655\n",
      "Epoch 3/100, Loss: 0.09848977625370026\n",
      "Epoch 4/100, Loss: 0.007795997895300388\n",
      "Epoch 5/100, Loss: 0.016726884990930557\n",
      "Epoch 6/100, Loss: 0.007545831613242626\n",
      "Epoch 7/100, Loss: 0.002078601624816656\n",
      "Epoch 8/100, Loss: 0.0025863463524729013\n",
      "Epoch 9/100, Loss: 0.000902361876796931\n",
      "Epoch 10/100, Loss: 0.0008003484690561891\n",
      "Epoch 11/100, Loss: 0.0011971123749390244\n",
      "Epoch 12/100, Loss: 0.00074769341154024\n",
      "Epoch 13/100, Loss: 0.0005101444548927248\n",
      "Epoch 14/100, Loss: 0.0004277488333173096\n",
      "Epoch 15/100, Loss: 0.0004083866660948843\n",
      "Epoch 16/100, Loss: 0.000583829649258405\n",
      "Epoch 17/100, Loss: 0.0004920220817439258\n",
      "Epoch 18/100, Loss: 0.0003246132982894778\n",
      "Epoch 19/100, Loss: 0.0005507726455107331\n",
      "Epoch 20/100, Loss: 0.00041075178887695074\n",
      "Epoch 21/100, Loss: 0.00027324981056153774\n",
      "Epoch 22/100, Loss: 0.0003000017604790628\n",
      "Epoch 23/100, Loss: 0.00024482584558427334\n",
      "Epoch 24/100, Loss: 0.0002473861677572131\n",
      "Epoch 25/100, Loss: 0.00023088170564733446\n",
      "Epoch 26/100, Loss: 0.0002546546165831387\n",
      "Epoch 27/100, Loss: 0.00023016601335257292\n",
      "Epoch 28/100, Loss: 0.0002393435570411384\n",
      "Epoch 29/100, Loss: 0.00013916712487116456\n",
      "Epoch 30/100, Loss: 0.00015674778842367232\n",
      "Epoch 31/100, Loss: 0.00018678349442780018\n",
      "Epoch 32/100, Loss: 0.00020668683282565325\n",
      "Epoch 33/100, Loss: 0.0001809432724257931\n",
      "Epoch 34/100, Loss: 0.00018106118659488857\n",
      "Epoch 35/100, Loss: 0.0001767713692970574\n",
      "Epoch 36/100, Loss: 0.0001516227057436481\n",
      "Epoch 37/100, Loss: 0.00014751045091543347\n",
      "Epoch 38/100, Loss: 0.00018624679069034755\n",
      "Epoch 39/100, Loss: 0.0001525760453660041\n",
      "Epoch 40/100, Loss: 0.0001764141779858619\n",
      "Epoch 41/100, Loss: 0.00018386334704700857\n",
      "Epoch 42/100, Loss: 0.00014983484288677573\n",
      "Epoch 43/100, Loss: 0.00018404191359877586\n",
      "Epoch 44/100, Loss: 0.00014953684876672924\n",
      "Epoch 45/100, Loss: 0.00013851160474587232\n",
      "Epoch 46/100, Loss: 0.00024762662360444665\n",
      "Epoch 47/100, Loss: 0.00016467407112941146\n",
      "Epoch 48/100, Loss: 0.000154960056534037\n",
      "Epoch 49/100, Loss: 0.00018523336621001363\n",
      "Epoch 50/100, Loss: 0.00012891623191535473\n",
      "Epoch 51/100, Loss: 0.00013815390411764383\n",
      "Epoch 52/100, Loss: 0.00013141959789209068\n",
      "Epoch 53/100, Loss: 0.00014029949670657516\n",
      "Epoch 54/100, Loss: 0.00012498305295594037\n",
      "Epoch 55/100, Loss: 0.00012528098886832595\n",
      "Epoch 56/100, Loss: 0.0001314791152253747\n",
      "Epoch 57/100, Loss: 0.00013851157564204186\n",
      "Epoch 58/100, Loss: 0.00014506696606986225\n",
      "Epoch 59/100, Loss: 0.00016413768753409386\n",
      "Epoch 60/100, Loss: 0.00013124081306159496\n",
      "Epoch 61/100, Loss: 0.00013189608580432832\n",
      "Epoch 62/100, Loss: 0.00013410142855718732\n",
      "Epoch 63/100, Loss: 0.00015138419985305518\n",
      "Epoch 64/100, Loss: 0.00012885690375696868\n",
      "Epoch 65/100, Loss: 0.00015037073171697557\n",
      "Epoch 66/100, Loss: 0.0001437560422345996\n",
      "Epoch 67/100, Loss: 0.00014953617937862873\n",
      "Epoch 68/100, Loss: 0.00012605587835423648\n",
      "Epoch 69/100, Loss: 0.00012200320634292439\n",
      "Epoch 70/100, Loss: 0.0001402398629579693\n",
      "Epoch 71/100, Loss: 0.00013648530875798315\n",
      "Epoch 72/100, Loss: 0.00014506655861623585\n",
      "Epoch 73/100, Loss: 0.00015466095646843314\n",
      "Epoch 74/100, Loss: 0.00011890419409610331\n",
      "Epoch 75/100, Loss: 0.00018654446466825902\n",
      "Epoch 76/100, Loss: 0.00011014330084435642\n",
      "Epoch 77/100, Loss: 0.00011806983093265444\n",
      "Epoch 78/100, Loss: 0.00013153866166248918\n",
      "Epoch 79/100, Loss: 0.00013898834004066885\n",
      "Epoch 80/100, Loss: 0.0001206921151606366\n",
      "Epoch 81/100, Loss: 0.00017265879432670772\n",
      "Epoch 82/100, Loss: 0.00011890419409610331\n",
      "Epoch 83/100, Loss: 0.00012188402615720406\n",
      "Epoch 84/100, Loss: 0.00017111000488512218\n",
      "Epoch 85/100, Loss: 0.00013886885426472872\n",
      "Epoch 86/100, Loss: 0.00012599624460563064\n",
      "Epoch 87/100, Loss: 0.00014250454842112958\n",
      "Epoch 88/100, Loss: 0.00013368416693992913\n",
      "Epoch 89/100, Loss: 0.0001513246970716864\n",
      "Epoch 90/100, Loss: 0.00012128781963838264\n",
      "Epoch 91/100, Loss: 0.00011830816220026463\n",
      "Epoch 92/100, Loss: 0.0001302872406085953\n",
      "Epoch 93/100, Loss: 0.00013428018428385258\n",
      "Epoch 94/100, Loss: 0.0001242679572897032\n",
      "Epoch 95/100, Loss: 0.0001551388413645327\n",
      "Epoch 96/100, Loss: 0.00013666409358847886\n",
      "Epoch 97/100, Loss: 0.00011091808846686035\n",
      "Epoch 98/100, Loss: 0.0001251021312782541\n",
      "Epoch 99/100, Loss: 0.00012587704986799508\n",
      "Epoch 100/100, Loss: 0.00016366070485673845\n",
      "Test Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# model parameter\n",
    "input_dim = len(vocab)\n",
    "model_dim = 128\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_classes = 2\n",
    "dropout = 0.1\n",
    "\n",
    "# model\n",
    "model = TransformerForClassification(input_dim, model_dim, num_layers, num_heads, ff_dim, num_classes, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (src, labels) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "train(model, train_loader, criterion, optimizer, num_epochs=100)\n",
    "\n",
    "# evaluate\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for src, labels in dataloader:\n",
    "            output = model(src)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "accuracy = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I really enjoyed this film' 's prediction type: 1\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "def predict(model, sentence, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    indices = sentence_to_indices(sentence, vocab)\n",
    "    indices = indices[:max_len]\n",
    "    indices += [vocab[\"<pad>\"]] * (max_len - len(indices))\n",
    "    src_tensor = torch.tensor(indices).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(src_tensor)\n",
    "        predicted_class = output.argmax(1).item()\n",
    "    \n",
    "    return predicted_class\n",
    "\n",
    "# example\n",
    "sentence = \"I really enjoyed this film\"\n",
    "predicted_class = predict(model, sentence, vocab)\n",
    "print(f\"'{sentence}' 's prediction type: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
